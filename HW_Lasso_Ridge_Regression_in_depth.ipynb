{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"},"colab":{"name":"Homework2.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"kO7UuwqdZlZQ"},"source":["# Домашнее задание №2: линейная регрессия (10 баллов).\n","\n","Некоторые задания будут по вариантам (всего 4 варианта). Чтобы выяснить свой вариант, посчитайте количество букв в своей фамилии, возьмете остаток от деления на 4 и прибавьте 1."]},{"cell_type":"code","metadata":{"id":"aK2FJen6ZlZS","executionInfo":{"status":"ok","timestamp":1601640610168,"user_tz":-180,"elapsed":1034,"user":{"displayName":"Анвар Хамидов","photoUrl":"","userId":"03201293090091994496"}}},"source":["import numpy as np\n","from numpy import linalg as LA"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ypY5t54PZlZW"},"source":["## Многомерная линейная регрессия из sklearn"]},{"cell_type":"markdown","metadata":{"id":"hMEFcokwZlZX"},"source":["Применим многомерную регрессию из sklearn для стандартного датасета"]},{"cell_type":"code","metadata":{"id":"Xf7nsS_vZlZX","executionInfo":{"status":"ok","timestamp":1601640610702,"user_tz":-180,"elapsed":1560,"user":{"displayName":"Анвар Хамидов","photoUrl":"","userId":"03201293090091994496"}},"outputId":"12a57e0a-be6b-44dc-9772-7918eed2fdce","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["from sklearn.datasets import make_regression\n","\n","X, y = make_regression(n_samples = 10000)\n","y = y.reshape(-1, 1)\n","print(X.shape, y.shape)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["(10000, 100) (10000, 1)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"aJGoksYhZlZa"},"source":["У нас 10000 объектов и 100 признаков. Для начала решим задачу аналитически \"из коробки\"."]},{"cell_type":"code","metadata":{"id":"wECJnNtzZlZa","executionInfo":{"status":"ok","timestamp":1601640611135,"user_tz":-180,"elapsed":1984,"user":{"displayName":"Анвар Хамидов","photoUrl":"","userId":"03201293090091994496"}},"outputId":"c664e845-fbfd-468d-8df3-ba408984149f","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import mean_squared_error\n","\n","reg = LinearRegression().fit(X, y)\n","mean_squared_error(y, reg.predict(X))"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1.707155037524044e-25"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"AiiYbE2aZlZd"},"source":["Теперь попробуем обучить линейную регрессию методом градиентного спуска \"из коробки\""]},{"cell_type":"code","metadata":{"id":"DaGMhmRmywy7","executionInfo":{"status":"ok","timestamp":1601640611136,"user_tz":-180,"elapsed":1977,"user":{"displayName":"Анвар Хамидов","photoUrl":"","userId":"03201293090091994496"}},"outputId":"a9dc7f03-8e32-4d28-fee3-d78825412e6e","colab":{"base_uri":"https://localhost:8080/","height":89}},"source":["from sklearn.linear_model import SGDRegressor\n","reg = SGDRegressor().fit(X, y)\n","mean_squared_error(y, reg.predict(X))"],"execution_count":4,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["0.00041907276059434883"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"z-DCaMZG0YUZ"},"source":["Как видим значение близко к LinearRegression"]},{"cell_type":"markdown","metadata":{"id":"wwS0P-gBZlZg"},"source":["***Задание 1 (1 балл).*** Объясните, чем вызвана разница в значениях двух полученных значений метрики?\n","\n","***Задание 2 (1 балл).*** Подберите гиперпараметры в методе градиентного спуска так, чтобы значение MSE было близко к значению MSE, полученному при обучении LinearRegression."]},{"cell_type":"markdown","metadata":{"id":"96YrR3YV0vsT"},"source":["Разница вызвана наличием регуляризации в SGD и тем, что SGD считает градиент по случайному наблюдению в то время как LinearRegresssion усредняет всю выборку. Но + SGD в его скорости."]},{"cell_type":"code","metadata":{"id":"Aqv4DuQHZlZd","executionInfo":{"status":"ok","timestamp":1601640611137,"user_tz":-180,"elapsed":1971,"user":{"displayName":"Анвар Хамидов","photoUrl":"","userId":"03201293090091994496"}},"outputId":"60c6a46d-b2e7-4ab2-bda2-fd523cdeb81a","colab":{"base_uri":"https://localhost:8080/","height":89}},"source":["reg1 = SGDRegressor(alpha = 0, tol = 1e-9, max_iter = 100000, learning_rate = 'constant', eta0 = 0.001).fit(X, y)\n","mean_squared_error(y, reg1.predict(X)) # В SGDRegressor по дефолту стоит регуляризация и если обнулить альфу, то это будет обычная регрессия\n"],"execution_count":5,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["2.7145737384275128e-25"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"uPpvTFnMZlZh"},"source":["## Ваша многомерная линейная регрессия"]},{"cell_type":"markdown","metadata":{"id":"C2fuiO_CZlZh"},"source":["***Задание 3 (5 баллов)***. Напишите собственную многомерную линейную регрессию, оптимизирующую MSE методом *градиентного спуска*. Для этого используйте шаблонный класс. \n","\n","Критерий останова: либо норма разности весов на текущей и предыдущей итерациях меньше определенного значения (первый и третий варианты), либо модуль разности функционалов качества (MSE) на текущей и предыдущей итерациях меньше определенного значения (второй и четвертый варианты). Также предлагается завершать обучение в любом случае, если было произведено слишком много итераций.\n","\n","***Задание 4 (2 балла)***. Добавьте l1 (первый и четвертый варианты) или l2 (второй и третий варианты) регуляризацию. "]},{"cell_type":"code","metadata":{"id":"E7PKYJ8YlcBz","executionInfo":{"status":"ok","timestamp":1601640611138,"user_tz":-180,"elapsed":1965,"user":{"displayName":"Анвар Хамидов","photoUrl":"","userId":"03201293090091994496"}},"outputId":"72c7737c-2496-41b5-f633-9595e02dd55a","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["X.shape, y.shape"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((10000, 100), (10000, 1))"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"xPr-EeKxqLwJ","executionInfo":{"status":"ok","timestamp":1601640611139,"user_tz":-180,"elapsed":1958,"user":{"displayName":"Анвар Хамидов","photoUrl":"","userId":"03201293090091994496"}}},"source":["X_new = np.concatenate([X, np.ones(X.shape[0]).reshape(-1, 1)], axis = 1) # В векторе Х нет константы, создадим ее"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"nvwVg1alZlZi","executionInfo":{"status":"ok","timestamp":1601640611140,"user_tz":-180,"elapsed":1954,"user":{"displayName":"Анвар Хамидов","photoUrl":"","userId":"03201293090091994496"}}},"source":["class LinearRegression(object):\n","    def __init__(self, alpha=0.01, l_ratio=0.001, tol=0.0001, max_iter=1000, lr_decay = 1, regularization = None, seed = 1):\n","        '''\n","        Для начала необходимо инициализировать параметры\n","        alpha - это learning rate или шаг обучения\n","        l_ratio - параметр регуляризации\n","        tol - значение для критерия останова\n","        max_iter - максимальное количество итераций обучения\n","        '''\n","        self.alpha = alpha\n","        self.l_ratio = l_ratio\n","        self.tol = tol\n","        self.max_iter = max_iter\n","        self.regularization = regularization\n","        self.lr_decay = lr_decay\n","\n","    def fit(self, X, y):\n","        '''\n","        Метод для обучения линейной регрессии\n","        X - матрица признаков\n","        y - вектор правильных ответов\n","        '''\n","        self.X = X\n","        self.y = y\n","        self.num_iter = 0\n","        self.w_0 = np.random.normal(0, 1, (self.X.shape[1], 1)) # инициализируем веса из нормального распределия N(0, 1)\n","        self.diff = 100000 # Инициализация для критерия останова\n","        \n","        #np.random.seed(123)\n","            \n","        if self.regularization == None:\n","            for i in range(self.max_iter): # Ограничение на число итераций\n","                if self.diff > self.tol: # Критерий останова\n","                    self.alpha = self.alpha * self.lr_decay # Адаптивный шаг градинта\n","\n","                    self.w_new = self.w_0 - self.alpha*2/self.X.shape[0]*np.dot(self.X.T, np.dot(self.X, self.w_0) - self.y) # Обновление градиента\n","            \n","                    self.diff =  LA.norm(self.w_new - self.w_0) # Подсчет нормы разности весов\n","\n","                    self.w_0 = self.w_new # Обновление W(k-1) для шага k\n","                    self.num_iter += 1\n","        elif self.regularization == 'l2':\n","            for i in range(self.max_iter): # Ограничение на число итераций\n","                if self.diff > self.tol: # Критерий останова\n","                    self.alpha = self.alpha * self.lr_decay # Адаптивный шаг градинта\n","             \n","                    self.w_new = self.w_0 - self.alpha*2/self.X.shape[0]*(np.dot(self.X.T, np.dot(self.X, self.w_0) - self.y) + self.l_ratio*self.w_0)\n","                    # Обновление градиента c учетом L2-регуляризации\n","            \n","                    self.diff =  LA.norm(self.w_new - self.w_0) # Подсчет нормы разности весов\n","\n","                    self.w_0 = self.w_new # Обновление W(k-1) для шага k\n","                    self.num_iter += 1\n","\n","        elif self.regularization == 'l1':\n","            for i in range(self.max_iter): # Ограничение на число итераций\n","                if self.diff > self.tol: # Критерий останова\n","                    self.alpha = self.alpha * self.lr_decay # Адаптивный шаг градинта\n","             \n","                    self.w_new = self.w_0 - self.alpha*2/self.X.shape[0]*np.dot(self.X.T, np.dot(self.X, self.w_0) - self.y) \n","                    + self.l_ratio*np.sign(self.w_0)\n","                    # Обновление градиента c учетом L1-регуляризации\n","            \n","                    self.diff =  LA.norm(self.w_new - self.w_0) # Подсчет нормы разности весов\n","\n","                    self.w_0 = self.w_new # Обновление W(k-1) для шага k\n","                    self.num_iter += 1\n","\n","                      \n","        return self.w_new \n","            \n","    def predict(self, X):\n","        '''\n","        Метод для предсказаний линейной регрессии\n","        X - матрица признаков\n","        '''\n","        self.y_pred = np.dot(self.X, self.w_new) # Предсказание модели\n","        \n","        return self.y_pred\n","        # your code here"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"CnxLk2Ba9kEo","executionInfo":{"status":"ok","timestamp":1601640611517,"user_tz":-180,"elapsed":2323,"user":{"displayName":"Анвар Хамидов","photoUrl":"","userId":"03201293090091994496"}},"outputId":"17e740ee-e7d3-41e2-e6b3-b15a3ca73fd3","colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["# Linear regression\n","my_reg = LinearRegression(regularization = None, lr_decay = 1)\n","my_reg.fit(X_new, y)\n","assert mean_squared_error(y, my_reg.predict(X_new)) < 1e-3\n","print('You are amazing! Great work!')\n","mean_squared_error(y, my_reg.predict(X_new))"],"execution_count":9,"outputs":[{"output_type":"stream","text":["You are amazing! Great work!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["2.6696364683813824e-05"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"ugtIniG5ZlZm","executionInfo":{"status":"ok","timestamp":1601640613105,"user_tz":-180,"elapsed":3906,"user":{"displayName":"Анвар Хамидов","photoUrl":"","userId":"03201293090091994496"}},"outputId":"891267f3-b0c6-4cac-be1a-ed47cabb0322","colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["# Lasso regression\n","my_reg = LinearRegression(regularization = 'l1', lr_decay = 1, l_ratio = 0.1)\n","my_reg.fit(X_new, y)\n","assert mean_squared_error(y, my_reg.predict(X_new)) < 1e-3\n","print('You are amazing! Great work!')\n","mean_squared_error(y, my_reg.predict(X_new))"],"execution_count":10,"outputs":[{"output_type":"stream","text":["You are amazing! Great work!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["2.71100584567059e-05"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"w15dL-Un9dez","executionInfo":{"status":"ok","timestamp":1601640613107,"user_tz":-180,"elapsed":3903,"user":{"displayName":"Анвар Хамидов","photoUrl":"","userId":"03201293090091994496"}},"outputId":"91517bd7-152e-4cf6-f87d-9207934f7d54","colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["# Ridge regression\n","my_reg = LinearRegression(regularization = 'l2', lr_decay =0.999, l_ratio = 0.1)\n","my_reg.fit(X_new, y)\n","assert mean_squared_error(y, my_reg.predict(X_new)) < 1e-3\n","print('You are amazing! Great work!')\n","mean_squared_error(y, my_reg.predict(X_new))"],"execution_count":11,"outputs":[{"output_type":"stream","text":["You are amazing! Great work!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["0.00016121958825897359"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"J_Y_ZCWIZlZo"},"source":["***Задание 5 (1 балл)***. Обучите линейную регрессию из коробки с l1-регуляризацией (from sklearn.linear_model import Lasso, первый вариант) или с l2-регуляризацией (from sklearn.linear_model import Ridge, второй вариант) с значением параметра регуляризации 0.1. Обучите вашу линейную регрессию с тем же значением параметра регуляризации и сравните результаты. Сделайте выводы."]},{"cell_type":"code","metadata":{"id":"9MG0u2quZlZp","executionInfo":{"status":"ok","timestamp":1601640613635,"user_tz":-180,"elapsed":4425,"user":{"displayName":"Анвар Хамидов","photoUrl":"","userId":"03201293090091994496"}},"outputId":"08e0a540-c5f4-49a4-b456-f66793d49c9a","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["from sklearn.linear_model import Lasso\n","lasso_reg = Lasso(alpha = 0.1).fit(X, y)\n","mean_squared_error(y, lasso_reg.predict(X))"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.09864602735396616"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"J1yCUmt9Cy-5","executionInfo":{"status":"ok","timestamp":1601640613636,"user_tz":-180,"elapsed":4421,"user":{"displayName":"Анвар Хамидов","photoUrl":"","userId":"03201293090091994496"}},"outputId":"c0785d45-3313-4885-ec97-db55af56f5c6","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["from sklearn.linear_model import Ridge\n","ridge_reg = Ridge(alpha = 0.1).fit(X, y)\n","mean_squared_error(y, ridge_reg.predict(X))"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["4.267555678619991e-06"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"qUlX4pQSLqGN"},"source":["Результаты написанной модели и моделей из коробки - похожие. В Lasso из коробки нельзя менять шаг спуска. В написанной модели есть адаптивный шаг спуска. Кроме этого я инициализирую веса изи нормального распределения для регрессии, Lasso, Ridge. Как это делается в коробке не нашел"]},{"cell_type":"markdown","metadata":{"id":"JAyeL-PPMLqZ"},"source":["Для Lasso из коробки MSE гораздо выше, чем для Ridge. Хотя Ridge вроде как сильнее штрафует за большие веса. C другой стороны Lasso хорошо работает в случае мультиколлинеарности, зануляя веса, что сильно может понизить MSE."]}]}